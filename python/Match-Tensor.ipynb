{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Match tensor\n",
    "## NOTE: no where close to fully functional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import merge, Dense, Input,Dropout, Embedding, LSTM, Bidirectional, Activation\n",
    "from keras.layers import Conv2D\n",
    "from keras.models import Model\n",
    "from HomeDepotCSVReader import HomeDepotReader\n",
    "import Utilities\n",
    "from DataPreprocessing import DataPreprocessing\n",
    "from Feature_Word2Vec import Feature_Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_filename = '../data/train_play.csv'\n",
    "test_filename = '../data/test_play.csv'\n",
    "attribute_filename = '../data/attributes_play.csv'\n",
    "description_filename = '../data/product_descriptions_play.csv'\n",
    "word2vec_model_path='model/word2vec_play.model'\n",
    "vocab_path='model/word2vec_play_vocab.json'\n",
    "embeddings_path='model/embeddings_play.npz'\n",
    "\n",
    "# train_filename = '../data/train.csv'\n",
    "# test_filename = '../data/test.csv'\n",
    "# attribute_filename = '../data/attributes.csv'\n",
    "# description_filename = '../data/product_descriptions.csv'\n",
    "# word2vec_model_path='model/word2vec.model'\n",
    "# vocab_path='model/word2vec_vocab.json'\n",
    "# embeddings_path='model/embeddings.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========Tranforming labels...\n",
      "showing current values\n",
      "trainDF: ['id', 'product_uid', 'search_term', 'relevance']\n",
      "trainDF:    id  product_uid    search_term  relevance\n",
      "0   2       100001  angle bracket        3.0\n",
      "self.mergedLabelDF: ['relevance'] \n",
      " <class 'pandas.core.frame.DataFrame'> (8, 1)    relevance\n",
      "0        3.0\n",
      "Old unique Labels: [ 2.33  2.5   2.67  3.  ]\n",
      "newLabels: [0 1 2 3]\n",
      "Creating new column for training:  relevance_int\n",
      "===========Transform labels completed\n",
      "train_query_df: ['id', 'product_uid', 'search_term', 'relevance', 'relevance_int', 'product_idx']\n",
      "product_df: ['product_title', 'product_uid', 'product_description']\n",
      "attribute_df: ['product_uid', 'name', 'value']\n",
      "test_query_df: ['id', 'product_uid', 'search_term']\n"
     ]
    }
   ],
   "source": [
    "reader = HomeDepotReader()\n",
    "\n",
    "train_query_df, product_df, attribute_df, test_query_df = reader.getQueryProductAttributeDataFrame(train_filename,\n",
    "                                              test_filename,\n",
    "                                              attribute_filename,\n",
    "                                              description_filename)\n",
    "print(\"train_query_df:\",list(train_query_df))\n",
    "print(\"product_df:\", list(product_df))\n",
    "print(\"attribute_df:\", list(attribute_df))\n",
    "print(\"test_query_df:\", list(test_query_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#transform attribute into doc\n",
    "dp = DataPreprocessing()\n",
    "attribute_doc_df = dp.getAttributeDoc(attribute_df)\n",
    "#attribute_doc_df\n",
    "product_df=product_df.join(attribute_doc_df.set_index('product_uid'), on = 'product_uid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Simpson Strong-Tie 12-Gauge Angle Not only do ...\n",
       "1    BEHR Premium Textured DeckOver 1-gal. #SC-141 ...\n",
       "2    Delta Vero 1-Handle Shower Only Faucet Trim Ki...\n",
       "3    Whirlpool 1.9 cu. ft. Over the Range Convectio...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_df['content'] = product_df['product_title'].map(str) + \" \" + \\\n",
    "                        product_df['product_description'].map(str) + \" \" + \\\n",
    "                        product_df['attr_json'].map(str)\n",
    "        \n",
    "# ## no attribute\n",
    "# product_df['content'] = product_df['product_title'].map(str) + \" \" + \\\n",
    "#                         product_df['product_description'].map(str) \n",
    "        \n",
    "product_df['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Word2Vec model\n",
      "Initialise/Train Word2Vec model\n",
      "('Time taken: ', 0, ' secs')\n",
      "Saving vectors to disk\n",
      "('Time taken: ', 0, ' secs')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "w2v=Feature_Word2Vec(modelFilename=word2vec_model_path)\n",
    "sentences=w2v.convertDFIntoSentences(product_df,'content')\n",
    "#print(sentences)\n",
    "w2v.trainModel(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#w2v.model.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  4.39748587e-03  -4.59543662e-03   2.49057752e-03   3.60534922e-03\n",
      "  -2.38019810e-03   4.06930828e-03  -2.67027761e-03  -4.66374727e-03\n",
      "   3.64948972e-03   4.73050540e-03   4.05373937e-03   3.57443443e-03\n",
      "   4.26247763e-03   3.43804527e-03  -3.13495588e-03  -2.91322678e-04\n",
      "  -1.73238001e-03  -3.92852956e-03  -3.71308601e-03  -3.26616759e-03\n",
      "   2.08726944e-03   2.94579938e-03  -1.19030941e-03   1.27816992e-03\n",
      "   4.69220430e-03  -2.72117741e-03   4.96241963e-03   3.35770706e-03\n",
      "  -1.85807655e-03  -5.19208238e-03  -4.44383593e-03  -2.32875906e-03\n",
      "   1.50145311e-03  -4.83125262e-03   2.06220429e-03  -2.43607000e-03\n",
      "  -3.38598201e-03   2.29467335e-03   2.38132942e-03   4.06370265e-03\n",
      "   5.48644166e-04  -6.11748037e-05  -8.98292870e-04  -1.57675601e-03\n",
      "  -3.61116906e-03  -4.53049364e-03   2.87177740e-04  -1.91700982e-03\n",
      "   3.72708659e-04   4.00600303e-03   4.19722823e-03  -1.39494997e-03\n",
      "  -3.38747050e-03   3.83756566e-03  -1.33652473e-03   2.14475556e-03\n",
      "   4.26710304e-03   2.53871456e-03  -2.15687766e-03   4.82586119e-03\n",
      "   2.22731871e-03  -4.73681092e-03   3.07790120e-03   3.13987862e-03\n",
      "   2.02664617e-03   1.56044349e-04   4.51417128e-03   1.01624915e-04\n",
      "  -3.95402312e-03  -2.10908893e-03   2.86473939e-03   1.25580060e-03\n",
      "   6.20266946e-04   5.79917280e-04  -3.89779569e-03  -7.90436927e-04\n",
      "  -1.69910630e-03  -9.04873814e-05  -1.48634054e-03   4.20703506e-03\n",
      "   2.93781911e-03  -2.24665971e-03   2.31982776e-04   5.14787855e-03\n",
      "  -3.03647877e-03   8.29026467e-05   1.21187000e-03  -2.11138115e-03\n",
      "   2.51187547e-03  -1.88673800e-03  -1.07603834e-03  -1.87098654e-03\n",
      "  -4.01294976e-03  -2.30641779e-03   3.53472569e-05   3.56063154e-03\n",
      "   5.48153999e-04   2.02512462e-03  -2.86712660e-03  -3.93143296e-03]\n",
      "[('ft', 0.27371764183044434), ('the', 0.240338996052742), ('from', 0.21540822088718414), ('fan', 0.20607316493988037), ('for', 0.19308435916900635)]\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "print(w2v.getVectorFromWord('wood'))\n",
    "print(w2v.getSimilarWordVectors('wood',5))\n",
    "print(len(w2v.getVectorFromWord('wood')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# embeddings to keras\n",
    "http://ben.bolte.cc/resources/embeddings/embeddings.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = dict([(k, v.index) for k, v in w2v.model.wv.vocab.items()])\n",
    "with open(vocab_path, 'w') as f:\n",
    "    f.write(json.dumps(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = w2v.model.wv.syn0\n",
    "np.save(open(embeddings_path, 'wb'), weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_vocab(vocab_path):\n",
    "    \"\"\"\n",
    "    Load word -> index and index -> word mappings\n",
    "    :param vocab_path: where the word-index map is saved\n",
    "    :return: word2idx, idx2word\n",
    "    \"\"\"\n",
    "\n",
    "    with open(vocab_path, 'r') as f:\n",
    "        data = json.loads(f.read())\n",
    "    word2idx = data\n",
    "    idx2word = dict([(v, k) for k, v in data.items()])\n",
    "    return word2idx, idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word2idx, idx2word = load_vocab(vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word2vec_embedding_layer(embeddings_path):\n",
    "    \"\"\"\n",
    "    Generate an embedding layer word2vec embeddings\n",
    "    :param embeddings_path: where the embeddings are saved (as a numpy file)\n",
    "    :return: the generated embedding layer\n",
    "    \"\"\"\n",
    "\n",
    "    weights = np.load(open(embeddings_path, 'rb'))\n",
    "    layer = Embedding(input_dim=weights.shape[0], output_dim=weights.shape[1], weights=[weights])\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print_tokens=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: a, variety, of, in, and, to, or, is, can, be, the, for, at, x, from, with, 'Number, 'Product, Depth, (in, )':, Height, Width, BEHR, It, your, wood, concrete, ft, 'Assembled, in',, 'Yes',, Type':, on, not, that, Sensor, this, microwave, cooking, have, you, time, interior, fast, fan\n"
     ]
    }
   ],
   "source": [
    "# variable arguments are passed to gensim's word2vec model\n",
    "# if options.train:\n",
    "#     print('Training Word2Vec...')\n",
    "#     create_embeddings(options.data, options.embeddings, options.vocab, size=100, min_count=5, window=5, sg=1, iter=25)\n",
    "\n",
    "word2idx, idx2word = load_vocab(vocab_path)\n",
    "\n",
    "if print_tokens:\n",
    "    print('Tokens:', ', '.join(word2idx.keys()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convert to idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def query_sent2idx(df,col):\n",
    "    query_word2vec_idx_list=[]\n",
    "    queries=w2v.convertDFIntoSentences(df,col)\n",
    "    print(len(queries))\n",
    "    for query in queries:\n",
    "        idx_list = []\n",
    "        for word in query:\n",
    "            if word not in word2idx.keys():\n",
    "                idx_list+=[len(word2idx.keys())] # use last as special key\n",
    "            else:\n",
    "                idx_list+=[word2idx[word]]\n",
    "        query_word2vec_idx_list+=[idx_list]\n",
    "        print(\"=====\")\n",
    "        print(idx_list)\n",
    "        print(\"=====\")\n",
    "    return query_word2vec_idx_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "=====\n",
      "[46, 46]\n",
      "=====\n",
      "=====\n",
      "[46, 46]\n",
      "=====\n",
      "=====\n",
      "[46, 46]\n",
      "=====\n",
      "=====\n",
      "[46, 46, 46]\n",
      "=====\n",
      "=====\n",
      "[46, 46, 46]\n",
      "=====\n",
      "=====\n",
      "[46, 46]\n",
      "=====\n",
      "=====\n",
      "[22, 46, 46]\n",
      "=====\n",
      "=====\n",
      "[46]\n",
      "=====\n",
      "[[46, 46], [46, 46], [46, 46], [46, 46, 46], [46, 46, 46], [46, 46], [22, 46, 46], [46]]\n"
     ]
    }
   ],
   "source": [
    "query_word2vec_idx_list = query_sent2idx(train_query_df,'search_term')\n",
    "print(query_word2vec_idx_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def doc_sent2idx(df,col):\n",
    "#     doc_word2vec_idx_list=[]\n",
    "#     for row in df.iteritems():\n",
    "#         a=w2v.convertDFIntoSentences(row,col)\n",
    "#         print(a)\n",
    "# #    print(len(queries))\n",
    "# #     for query in queries:\n",
    "# #         idx_list = []\n",
    "# #         for word in query:\n",
    "# #             if word not in word2idx.keys():\n",
    "# #                 idx_list+=[len(word2idx.keys())] # use last as special key\n",
    "# #             else:\n",
    "# #                 idx_list+=[word2idx[word]]\n",
    "# #         query_word2vec_idx_list+=[idx_list]\n",
    "# #         print(\"=====\")\n",
    "# #         print(idx_list)\n",
    "# #         print(\"=====\")\n",
    "#     return doc_word2vec_idx_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "=====\n",
      "[46, 46, 46, 46]\n",
      "=====\n",
      "=====\n",
      "[40, 46, 46, 46, 46]\n",
      "=====\n",
      "=====\n",
      "[46, 46, 46, 0, 46, 46]\n",
      "=====\n",
      "=====\n",
      "[46, 46, 46, 46, 46, 46, 46, 46, 6, 46, 46, 46, 46]\n",
      "=====\n",
      "=====\n",
      "[46, 46]\n",
      "=====\n",
      "=====\n",
      "[46, 46]\n",
      "=====\n",
      "=====\n",
      "[18]\n",
      "=====\n",
      "=====\n",
      "[46, 1, 46, 46, 46, 6, 46, 46, 7, 33, 46]\n",
      "=====\n",
      "[[46, 46, 46, 46], [40, 46, 46, 46, 46], [46, 46, 46, 0, 46, 46], [46, 46, 46, 46, 46, 46, 46, 46, 6, 46, 46, 46, 46], [46, 46], [46, 46], [18], [46, 1, 46, 46, 46, 6, 46, 46, 7, 33, 46]]\n"
     ]
    }
   ],
   "source": [
    "#TODO: this is fucked. just bodge for testing nn\n",
    "doc_word2vec_idx_list = query_sent2idx(product_df,'product_title')\n",
    "print(doc_word2vec_idx_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # cosine similarity model\n",
    "# print('Building model...')\n",
    "# input_a = Input(shape=(1,), dtype='int32', name='input_a')\n",
    "# input_b = Input(shape=(1,), dtype='int32', name='input_b')\n",
    "# embeddings = word2vec_embedding_layer(embeddings_path)\n",
    "# embedding_a = embeddings(input_a)\n",
    "# embedding_b = embeddings(input_b)\n",
    "# similarity = merge([embedding_a, embedding_b], mode='cos', dot_axes=2)\n",
    "# model = Model(input=[input_a, input_b], output=similarity)\n",
    "# model.compile(optimizer='sgd', loss='mse') # optimizer and loss don't matter\n",
    "\n",
    "\n",
    "# word_a = 'wood'#raw_input('First word: ')\n",
    "# if word_a not in word2idx:\n",
    "#     print('\"%s\" is not in the index' % word_a)\n",
    "# word_b = 'fan'#raw_input('Second word: ')\n",
    "# if word_b not in word2idx:\n",
    "#     print('\"%s\" is not in the index' % word_b)\n",
    "# output = model.predict([np.asarray([word2idx[word_a]]), np.asarray([word2idx[word_b]])])\n",
    "# print('%f' % output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query_max_len = 6 #covers 95.74% of the search lengths (see data exploration)\n",
    "doc_max_len = 400 # todo: confirm this is sensible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_query (InputLayer)         (None, 6)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)          (None, 6, 100)        4600        input_query[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dense_11 (Dense)                 (None, 6, 40)         4040        embedding_5[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional)  (None, 60)            17040       dense_11[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_12 (Dense)                 (None, 50)            3050        bidirectional_5[0][0]            \n",
      "====================================================================================================\n",
      "Total params: 28,730\n",
      "Trainable params: 28,730\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print('Building model...')\n",
    "embeddings = word2vec_embedding_layer(embeddings_path)\n",
    "\n",
    "#embedding lookup\n",
    "input_query = Input(shape=(query_max_len,), dtype='int32', name='input_query')\n",
    "#input_doc = Input(shape=(doc_max_len,), dtype='int32', name='input_doc')\n",
    "embedding_query = embeddings(input_query)  # (None, 6, 100)\n",
    "#embedding_doc = embeddings(input_doc)      # (None, 400, 100)\n",
    "\n",
    "#shared linear projection\n",
    "shared_lp = Dense(40,activation='linear')\n",
    "query_output = shared_lp(embedding_query) # (None, 6, 40) \n",
    "#doc_output = shared_lp(embedding_doc) #(None, 400, 40) \n",
    "\n",
    "#TODO: fix output shape here, not right. should be (None, 6, 30) for query\n",
    "#query: bi LSTM, lp\n",
    "query_output = Bidirectional(LSTM(30))(query_output) #(None, 60) #bidirectional x2\n",
    "query_output = Dense(50,activation='linear')(query_output) #(None, 50)\n",
    "\n",
    "#doc: bi LSTM, lp\n",
    "#doc_output = Bidirectional(LSTM(140))(doc_output) #(None, 280) #bidirectional x2\n",
    "#doc_output = Dense(50,activation='linear')(doc_output) #(None, 50)\n",
    "\n",
    "#2d product\n",
    "#output = merge([query_output, doc_output], mode='mul') #(None, 50) #TODO: verify this shape\n",
    "\n",
    "#Append exact match channel\n",
    "#TODO:\n",
    "\n",
    "# Conv layer\n",
    "\n",
    "# Final layer\n",
    "#output = Dense(1,activation='sigmoid')(output)\n",
    "\n",
    "# build model\n",
    "#model = Model([input_query], [encoded_query])\n",
    "#model = Model([input_doc], [encoded_doc])\n",
    "#model = Model([input_query,input_doc], [output])\n",
    "model = Model([input_query,input_doc], [query_output])\n",
    "#model = Model([input_query,input_doc], [query_output,doc_output])\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "x_query_train shape: (8, 6)\n",
      "x_doc_train shape: (8, 400)\n"
     ]
    }
   ],
   "source": [
    "x_query_train=np.array(query_word2vec_idx_list)\n",
    "x_doc_train=np.array(doc_word2vec_idx_list)\n",
    "\n",
    "print(\"Pad sequences (samples x time)\")\n",
    "x_query_train = sequence.pad_sequences(x_query_train, maxlen=query_max_len)\n",
    "x_doc_train = sequence.pad_sequences(x_doc_train, maxlen=doc_max_len)\n",
    "# x_query_test = sequence.pad_sequences(x_query_test, maxlen=query_max_len)\n",
    "# x_doc_test = sequence.pad_sequences(x_doc_test, maxlen=doc_max_len)\n",
    "\n",
    "print('x_query_train shape:', x_query_train.shape)\n",
    "print('x_doc_train shape:', x_doc_train.shape)\n",
    "# print('x_query_test shape:', x_query_test.shape)\n",
    "# print('x_doc_test shape:', x_doc_test.shape)\n",
    "\n",
    "y_train=train_query_df['relevance_int'].as_matrix()\n",
    "# y_test=test_query_df['relevance_int'].as_matrix()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking model target: expected embedding_2 to have 3 dimensions, but got array with shape (8, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-b19058261e74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m hist=model.fit([x_query_train,x_doc_train], [y_train],\n\u001b[0;32m----> 4\u001b[0;31m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m           \u001b[0;31m#epochs=4,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m           \u001b[0;31m#validation_data=[x_test, y_test])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ongmin/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch)\u001b[0m\n\u001b[1;32m   1114\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1116\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1117\u001b[0m         \u001b[0;31m# prepare validation data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ongmin/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[1;32m   1031\u001b[0m                                    \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m                                    \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m                                    exception_prefix='model target')\n\u001b[0m\u001b[1;32m   1034\u001b[0m         sample_weights = standardize_sample_weights(sample_weight,\n\u001b[1;32m   1035\u001b[0m                                                     self.output_names)\n",
      "\u001b[0;32m/home/ongmin/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    110\u001b[0m                                  \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                                  \u001b[0;34m' dimensions, but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m                                  str(array.shape))\n\u001b[0m\u001b[1;32m    113\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_dim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking model target: expected embedding_2 to have 3 dimensions, but got array with shape (8, 1)"
     ]
    }
   ],
   "source": [
    "batch_size=2\n",
    "print('Train...')\n",
    "hist=model.fit([x_query_train,x_doc_train], [y_train],\n",
    "          batch_size=batch_size,\n",
    "          #epochs=4,\n",
    "          #validation_data=[x_test, y_test])\n",
    "         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([x_query_train,x_doc_train],\n",
    "          batch_size=batch_size,\n",
    "          #epochs=4,\n",
    "          #validation_data=[x_test, y_test])\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking : expected input_query to have shape (None, 6) but got array with shape (1, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-1f5d33ee459a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword_b\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword2idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\"%s\" is not in the index'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword2idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_a\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword2idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_b\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ongmin/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[1;32m   1252\u001b[0m         x = standardize_input_data(x, self.input_names,\n\u001b[1;32m   1253\u001b[0m                                    \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1254\u001b[0;31m                                    check_batch_axis=False)\n\u001b[0m\u001b[1;32m   1255\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ongmin/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    122\u001b[0m                             \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m                             \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m                             str(array.shape))\n\u001b[0m\u001b[1;32m    125\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking : expected input_query to have shape (None, 6) but got array with shape (1, 1)"
     ]
    }
   ],
   "source": [
    "print(\"Predict\")\n",
    "word_a = 'wood'#raw_input('First word: ')\n",
    "if word_a not in word2idx:\n",
    "    print('\"%s\" is not in the index' % word_a)\n",
    "word_b = 'fan'#raw_input('Second word: ')\n",
    "if word_b not in word2idx:\n",
    "    print('\"%s\" is not in the index' % word_b)\n",
    "output = model.predict([np.asarray([word2idx[word_a]]), np.asarray([word2idx[word_b]])])\n",
    "print('%f' % output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
