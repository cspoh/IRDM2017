{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Match tensor\n",
    "## NOTE: no where close to fully functional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import merge, Dense, Input,Dropout, Embedding, LSTM, Bidirectional, Activation\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers.merge import dot, multiply, add, concatenate\n",
    "from keras.layers import Merge\n",
    "from keras.layers.core import Lambda,Reshape, Flatten\n",
    "from keras.layers.pooling import GlobalMaxPooling2D\n",
    "from keras.models import Model\n",
    "from keras.backend import transpose,batch_dot,expand_dims\n",
    "from keras import optimizers\n",
    "from HomeDepotCSVReader import HomeDepotReader\n",
    "import Utilities\n",
    "from DataPreprocessing import DataPreprocessing\n",
    "from Feature_Word2Vec import Feature_Word2Vec\n",
    "from AutomaticQueryExpansion import Word2VecQueryExpansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_filename = '../data/train_play.csv'\n",
    "# test_filename = '../data/test_play.csv'\n",
    "# attribute_filename = '../data/attributes_play.csv'\n",
    "# description_filename = '../data/product_descriptions_play.csv'\n",
    "# word2vec_model_path='model/word2vec_play.model'\n",
    "# vocab_path='model/word2vec_play_vocab.json'\n",
    "# embeddings_path='model/embeddings_play.npz'\n",
    "\n",
    "train_filename = '../data/train.csv'\n",
    "test_filename = '../data/test.csv'\n",
    "attribute_filename = '../data/attributes.csv'\n",
    "description_filename = '../data/product_descriptions.csv'\n",
    "word2vec_model_path='model/word2vec.model'\n",
    "vocab_path='model/word2vec_vocab.json'\n",
    "embeddings_path='model/embeddings.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========Tranforming labels...\n",
      "showing current values\n",
      "trainDF: ['id', 'product_uid', 'search_term', 'relevance']\n",
      "trainDF:    id  product_uid    search_term  relevance\n",
      "0   2       100001  angle bracket        3.0\n",
      "self.mergedLabelDF: ['relevance'] \n",
      " <class 'pandas.core.frame.DataFrame'> (74067, 1)    relevance\n",
      "0        3.0\n",
      "Old unique Labels: [ 1.    1.25  1.33  1.5   1.67  1.75  2.    2.25  2.33  2.5   2.67  2.75\n",
      "  3.  ]\n",
      "newLabels: [ 0  1  2  3  4  5  6  7  8  9 10 11 12]\n",
      "Creating new column for training:  relevance_int\n",
      "===========Transform labels completed\n",
      "train_query_df: ['id', 'product_uid', 'search_term', 'relevance', 'relevance_int', 'product_idx']\n",
      "product_df: ['product_title', 'product_uid', 'product_description']\n",
      "attribute_df: ['product_uid', 'name', 'value']\n",
      "test_query_df: ['id', 'product_uid', 'search_term']\n"
     ]
    }
   ],
   "source": [
    "reader = HomeDepotReader()\n",
    "\n",
    "train_query_df, product_df, attribute_df, test_query_df = reader.getQueryProductAttributeDataFrame(train_filename,\n",
    "                                              test_filename,\n",
    "                                              attribute_filename,\n",
    "                                              description_filename)\n",
    "print(\"train_query_df:\",list(train_query_df))\n",
    "print(\"product_df:\", list(product_df))\n",
    "print(\"attribute_df:\", list(attribute_df))\n",
    "print(\"test_query_df:\", list(test_query_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#transform attribute into doc\n",
    "dp = DataPreprocessing()\n",
    "attribute_doc_df = dp.getAttributeDoc(attribute_df)\n",
    "#attribute_doc_df\n",
    "product_df=product_df.join(attribute_doc_df.set_index('product_uid'), on = 'product_uid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# product_df['content'] = train_query_df['search_term'].map(str) + \" \" + \\\n",
    "#                         product_df['product_title'].map(str) + \" \" + \\\n",
    "#                         product_df['product_description'].map(str) + \" \" + \\\n",
    "#                         product_df['attr_json'].map(str)\n",
    "\n",
    "product_df['content'] = product_df['product_title'].map(str) + \" \" + \\\n",
    "                        product_df['product_description'].map(str) \n",
    "\n",
    "# ## no attribute\n",
    "# product_df['content'] = product_df['product_title'].map(str) + \" \" + \\\n",
    "#                         product_df['product_description'].map(str) \n",
    "        \n",
    "#product_df['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Word2Vec model\n",
      "Initialise/Train Word2Vec model\n",
      "('Time taken: ', 1.5333333333333334, ' mins')\n",
      "Saving vectors to disk\n",
      "('Time taken: ', 0, ' secs')\n"
     ]
    }
   ],
   "source": [
    "#w2v=Feature_Word2Vec(modelFilename=word2vec_model_path)#modelFilename=word2vec_model_path\n",
    "w2v=Feature_Word2Vec()\n",
    "sentences=w2v.convertDFIntoSentences(product_df,'content')\n",
    "#print(sentences)\n",
    "w2v.trainModel(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72827"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(w2v.model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# w2vExpand=Word2VecQueryExpansion(modelFilename=word2vec_model_path)\n",
    "# query=\"cooking\"\n",
    "# print(\"Expanding query: \")\n",
    "# print(w2vExpand.getExpandedQuery(query,maxNoOfAdditionalWords=2,minSimilarityLevel=0.65,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4.05043507  3.49019146 -0.06090352  1.71556103  2.35107517  2.13475037\n",
      " -1.92377174 -3.25637412  0.56044257 -0.40419373  1.90303624 -0.17706122\n",
      "  2.54476619 -1.1679852  -1.04832709  0.46012864  1.18832374  1.28595293\n",
      "  6.29103231  0.77106035 -4.03227377  1.97043014 -0.59022874 -3.17424178\n",
      " -0.36804637 -0.61262178 -0.75078177 -0.42666331  2.30822086 -4.05024481\n",
      "  0.72244686  1.72640133 -1.25437295 -1.05009019 -2.08370948 -0.85143971\n",
      " -0.56405699  4.1177001   2.61707401  1.55357254 -2.06696749 -2.00102615\n",
      "  0.48281354 -1.68158555  2.12631869  3.89384604 -0.43447676  0.97610545\n",
      " -2.19433045 -1.21479046 -4.6699028  -2.06615448  4.94874239 -0.38742435\n",
      " -2.90473318  2.34807134  2.0020256  -0.78927368 -0.1993627   2.29082036\n",
      "  3.06380582  1.93890679  1.09457994  0.46953908  2.76734853  2.98501611\n",
      " -2.73549151 -1.32382369 -2.61912513 -0.74327958 -2.24027848  1.3900671\n",
      " -1.26408017  0.72640014  0.88561004  2.5344274  -3.36937737  2.88120604\n",
      " -1.90230823  3.23573303 -3.50353646 -0.34555268 -0.22222079  0.41512179\n",
      " -0.71926993  1.23009717  0.44523373 -1.96967781  1.04982185 -0.44426966\n",
      " -1.80426645 -3.14278293 -0.4707236   2.73234773  1.31908238  2.57043123\n",
      "  1.53224754 -0.30132163  1.70937884  0.81213009]\n",
      "[('hardwood', 0.6790220141410828), ('hardwoods', 0.583315372467041), ('wooden', 0.5643646121025085), ('cedar', 0.5568280220031738), ('pine', 0.5563942790031433)]\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "print(w2v.getVectorFromWord('wood'))\n",
    "print(w2v.getSimilarWordVectors('wood',5))\n",
    "print(len(w2v.getVectorFromWord('wood')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# embeddings to keras\n",
    "http://ben.bolte.cc/resources/embeddings/embeddings.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = dict([(k, v.index) for k, v in w2v.model.wv.vocab.items()])\n",
    "with open(vocab_path, 'w') as f:\n",
    "    f.write(json.dumps(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = w2v.model.wv.syn0\n",
    "np.save(open(embeddings_path, 'wb'), weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_vocab(vocab_path):\n",
    "    \"\"\"\n",
    "    Load word -> index and index -> word mappings\n",
    "    :param vocab_path: where the word-index map is saved\n",
    "    :return: word2idx, idx2word\n",
    "    \"\"\"\n",
    "\n",
    "    with open(vocab_path, 'r') as f:\n",
    "        data = json.loads(f.read())\n",
    "    word2idx = data\n",
    "    idx2word = dict([(v, k) for k, v in data.items()])\n",
    "    return word2idx, idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word2idx, idx2word = load_vocab(vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word2vec_embedding_layer(embeddings_path):\n",
    "    \"\"\"\n",
    "    Generate an embedding layer word2vec embeddings\n",
    "    :param embeddings_path: where the embeddings are saved (as a numpy file)\n",
    "    :return: the generated embedding layer\n",
    "    \"\"\"\n",
    "\n",
    "    weights = np.load(open(embeddings_path, 'rb'))\n",
    "    layer = Embedding(input_dim=weights.shape[0], output_dim=weights.shape[1], weights=[weights])\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print_tokens=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# variable arguments are passed to gensim's word2vec model\n",
    "# if options.train:\n",
    "#     print('Training Word2Vec...')\n",
    "#     create_embeddings(options.data, options.embeddings, options.vocab, size=100, min_count=5, window=5, sg=1, iter=25)\n",
    "\n",
    "word2idx, idx2word = load_vocab(vocab_path)\n",
    "\n",
    "if print_tokens:\n",
    "    print('Tokens:', ', '.join(word2idx.keys()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convert to idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def query_sent2idx(df,col):\n",
    "    query_word2vec_idx_list=[]\n",
    "    queries=w2v.convertDFIntoSentences(df,col)\n",
    "    print(len(queries))\n",
    "    for query in queries:\n",
    "        idx_list = []\n",
    "        for word in query:\n",
    "            if word not in word2idx.keys():\n",
    "                idx_list+=[0]#[len(word2idx.keys())] # use last as special key #TODO: well we need to fix this. Using 0 for now so it's in range To OOV or something random\n",
    "            else:\n",
    "                idx_list+=[word2idx[word]]\n",
    "        query_word2vec_idx_list+=[idx_list]\n",
    "        #print(\"=====\")\n",
    "        #print(idx_list)\n",
    "        #print(\"=====\")\n",
    "    return query_word2vec_idx_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75575\n"
     ]
    }
   ],
   "source": [
    "query_word2vec_idx_list = query_sent2idx(train_query_df,'search_term')\n",
    "#print(query_word2vec_idx_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def doc_sent2idx(df,col):\n",
    "#     doc_word2vec_idx_list=[]\n",
    "#     for row in df.iteritems():\n",
    "#         a=w2v.convertDFIntoSentences(row,col)\n",
    "#         print(a)\n",
    "# #    print(len(queries))\n",
    "# #     for query in queries:\n",
    "# #         idx_list = []\n",
    "# #         for word in query:\n",
    "# #             if word not in word2idx.keys():\n",
    "# #                 idx_list+=[len(word2idx.keys())] # use last as special key\n",
    "# #             else:\n",
    "# #                 idx_list+=[word2idx[word]]\n",
    "# #         query_word2vec_idx_list+=[idx_list]\n",
    "# #         print(\"=====\")\n",
    "# #         print(idx_list)\n",
    "# #         print(\"=====\")\n",
    "#     return doc_word2vec_idx_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "277395\n"
     ]
    }
   ],
   "source": [
    "#TODO: this is fucked. just bodge for testing nn\n",
    "doc_word2vec_idx_list = query_sent2idx(product_df,'product_title')\n",
    "#print(doc_word2vec_idx_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # cosine similarity model\n",
    "# print('Building model...')\n",
    "# input_a = Input(shape=(1,), dtype='int32', name='input_a')\n",
    "# input_b = Input(shape=(1,), dtype='int32', name='input_b')\n",
    "# embeddings = word2vec_embedding_layer(embeddings_path)\n",
    "# embedding_a = embeddings(input_a)\n",
    "# embedding_b = embeddings(input_b)\n",
    "# similarity = merge([embedding_a, embedding_b], mode='cos', dot_axes=2)\n",
    "# model = Model(input=[input_a, input_b], output=similarity)\n",
    "# model.compile(optimizer='sgd', loss='mse') # optimizer and loss don't matter\n",
    "\n",
    "\n",
    "# word_a = 'wood'#raw_input('First word: ')\n",
    "# if word_a not in word2idx:\n",
    "#     print('\"%s\" is not in the index' % word_a)\n",
    "# word_b = 'fan'#raw_input('Second word: ')\n",
    "# if word_b not in word2idx:\n",
    "#     print('\"%s\" is not in the index' % word_b)\n",
    "# output = model.predict([np.asarray([word2idx[word_a]]), np.asarray([word2idx[word_b]])])\n",
    "# print('%f' % output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "query_max_len = 6 #covers 95.74% of the search lengths (see data exploration)\n",
    "# find longest sub list\n",
    "doc_max_len = len(max(doc_word2vec_idx_list,key=len)) #400 # todo: confirm this is sensible\n",
    "print(doc_max_len)\n",
    "print(query_max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_query (InputLayer)         (None, 6)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_doc (InputLayer)           (None, 27)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)          multiple              7282700                                      \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  multiple              4040                                         \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional)  (None, 6, 30)         17040                                        \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional)  (None, 27, 30)        17040                                        \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 6, 50)         1550                                         \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 27, 50)        1550                                         \n",
      "____________________________________________________________________________________________________\n",
      "input_exact_match (InputLayer)   (None, 6, 27)         0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dot_1 (Dot)                      (None, 6, 27)         0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 6, 27)         756                                          \n",
      "____________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)                (None, 6, 27, 1)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)                (None, 6, 27, 1)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 6, 27, 2)      0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)                (None, 4, 25, 18)     342                                          \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)                (None, 4, 24, 18)     450                                          \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)                (None, 4, 23, 18)     558                                          \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)                (None, 4, 25, 20)     380                                          \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)                (None, 4, 24, 20)     380                                          \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)                (None, 4, 23, 20)     380                                          \n",
      "____________________________________________________________________________________________________\n",
      "global_max_pooling2d_1 (GlobalMa (None, 20)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "global_max_pooling2d_2 (GlobalMa (None, 20)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "global_max_pooling2d_3 (GlobalMa (None, 20)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "add_1 (Add)                      (None, 20)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_5 (Dense)                  (None, 1)             21                                           \n",
      "====================================================================================================\n",
      "Total params: 7,327,187\n",
      "Trainable params: 7,327,187\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print('Building model...')\n",
    "\n",
    "def get_R(x):\n",
    "    a, b = x.values()\n",
    "    return K.dot(a, b)\n",
    "\n",
    "\n",
    "embeddings = word2vec_embedding_layer(embeddings_path)\n",
    "\n",
    "#embedding lookup\n",
    "# TODO: replace this with embedding with mask zero rather than padding, need to change index for encoding too, imput dim too.\n",
    "# TODO: OOV embedding\n",
    "input_query = Input(shape=(query_max_len,), dtype='int32', name='input_query')\n",
    "input_doc = Input(shape=(doc_max_len,), dtype='int32', name='input_doc')\n",
    "input_exact_match = Input(shape=(query_max_len,doc_max_len), dtype='float32', name='input_exact_match')\n",
    "embedding_query = embeddings(input_query)  # (None, 6, 100)\n",
    "embedding_doc = embeddings(input_doc)      # (None, 400, 100)\n",
    "\n",
    "#shared linear projection\n",
    "shared_lp = Dense(40,activation='linear')\n",
    "query_output = shared_lp(embedding_query) # (None, 6, 40) \n",
    "doc_output = shared_lp(embedding_doc) #(None, 400, 40) \n",
    "\n",
    "#query: bi LSTM, lp -- implementation = 0 for CPU option, 1 or 2 for GPU\n",
    "query_output = Bidirectional(LSTM(30, dropout=0.5, implementation=0, return_sequences=True, go_backwards=True)\\\n",
    "                             ,merge_mode='mul'#TODO: alts are 'sum','mul','ave','concat'<--default, None\n",
    "                            )(query_output) #(None, 6, 30) unless concat (None, 6, 60)  \n",
    "query_output = Dense(50,activation='linear')(query_output) #(None, 50)\n",
    "\n",
    "#doc: bi LSTM, lp\n",
    "doc_output = Bidirectional(LSTM(30, dropout=0.5, implementation=0, return_sequences=True, go_backwards=True)\\\n",
    "                             ,merge_mode='mul'#TODO: alts are 'sum','mul','ave','concat'<--default, None\n",
    "                            )(doc_output) #(None, 6, 400) unless concat (None, 6, 800)  \n",
    "doc_output = Dense(50,activation='linear')(doc_output) #(None, 50)\n",
    "\n",
    "#2d product\n",
    "#mt_input = batch_dot(query_output,doc_output,axes=None) #axes=[2,2])\n",
    "#query_output = Flatten()(query_output)\n",
    "#doc_output = Flatten()(doc_output)\n",
    "#mt_input = multiply([query_output, doc_output])\n",
    "#mt_input = dot([query_output, doc_output], axes=(0), normalize=False)  \n",
    "dot_product_output = dot([query_output, doc_output], axes=(2), normalize=True)  \n",
    "#output is (11, 6, 400), where did 50 go?\n",
    "def func_expand_dims(x):\n",
    "    return expand_dims(x, axis=-1)\n",
    "\n",
    "def expand_dims_output_shape(input_shape):\n",
    "    return (input_shape[0], input_shape[1],input_shape[2],1)\n",
    "\n",
    "mt_input_1 = Lambda(func_expand_dims, expand_dims_output_shape)(dot_product_output)\n",
    "\n",
    "#mt_input = dot([transpose(query_output), transpose(doc_output)], axes=(0), normalize=False)  \n",
    "######################################\n",
    "\n",
    "def state_layer_dot_prod(x):\n",
    "    output_list=[]\n",
    "#    output = dot([x[0], x[1]], axes=(2), normalize=False)  \n",
    "    for i in range(50):\n",
    "        print(i)\n",
    "        print(x[0][i])\n",
    "        print(x[1][i])\n",
    "        #output = multiply([transpose(x[0][i]), x[1][i]])\n",
    "        #output_list += output\n",
    "        #output = dot([x[0][i], transpose(x[1][i])])\n",
    "        #transpose(x[1][i])\n",
    "    print(output_list)\n",
    "    return output_list\n",
    "\n",
    "def state_layer_dot_prod_shape(input_shape):\n",
    "    'Merge output shape'\n",
    "    shape = list(input_shape)\n",
    "    #print(input_shape)\n",
    "    #print(shape)\n",
    "    outshape = (shape[0][0],shape[1][1],shape[0][1],shape[0][2])\n",
    "    #print(outshape)\n",
    "    return tuple(outshape)\n",
    "\n",
    "######################################\n",
    "#mt_input = Lambda(state_layer_dot_prod,output_shape=state_layer_dot_prod_shape)([transpose(query_output), transpose(doc_output)])\n",
    "#mt_input = Lambda(state_layer_dot_prod,output_shape=state_layer_dot_prod_shape)([query_output, doc_output])\n",
    "\n",
    "##TODO fix error:\n",
    "#InvalidArgumentError: Input to reshape is a tensor with 4800 values, but the requested shape requires a multiple of 120000\n",
    "#[[Node: reshape_23/Reshape = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](lambda_40/dot_42/MatMul, reshape_23/Reshape/shape)]]\n",
    "#mt_input = Reshape((doc_max_len, query_max_len,50))(mt_input)\n",
    "\n",
    "#Append exact match channel\n",
    "output_exact_match = Dense(doc_max_len,activation='linear')(input_exact_match)\n",
    "mt_input_2 = Lambda(func_expand_dims, expand_dims_output_shape)(output_exact_match)\n",
    "# mt_input = concatenate([mt_input_1,mt_input_2],axis=0)\n",
    "mt_input = concatenate([mt_input_1,mt_input_2],axis=3)\n",
    "\n",
    "# Conv layers 1\n",
    "output1 = Conv2D(filters=18, kernel_size=(3, 3), strides=(1,1), padding='valid', data_format='channels_last', \\\n",
    "                dilation_rate=(1, 1), activation='relu', use_bias=True, kernel_initializer='glorot_uniform', \\\n",
    "                bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, \\\n",
    "                activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(mt_input)\n",
    "\n",
    "output2 = Conv2D(filters=18, kernel_size=(3, 4), strides=(1,1), padding='valid', data_format='channels_last', \\\n",
    "                dilation_rate=(1, 1), activation='relu', use_bias=True, kernel_initializer='glorot_uniform', \\\n",
    "                bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, \\\n",
    "                activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(mt_input)\n",
    "\n",
    "output3 = Conv2D(filters=18, kernel_size=(3, 5), strides=(1,1), padding='valid', data_format='channels_last', \\\n",
    "                dilation_rate=(1, 1), activation='relu', use_bias=True, kernel_initializer='glorot_uniform', \\\n",
    "                bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, \\\n",
    "                activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(mt_input)\n",
    "\n",
    "# Conv layers 2\n",
    "output1 = Conv2D(filters=20, kernel_size=(1, 1), strides=(1,1), padding='valid', data_format='channels_last', \\\n",
    "                dilation_rate=(1, 1), activation='relu', use_bias=True, kernel_initializer='glorot_uniform', \\\n",
    "                bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, \\\n",
    "                activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(output1)\n",
    "\n",
    "output2 = Conv2D(filters=20, kernel_size=(1, 1), strides=(1,1), padding='valid', data_format='channels_last', \\\n",
    "                dilation_rate=(1, 1), activation='relu', use_bias=True, kernel_initializer='glorot_uniform', \\\n",
    "                bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, \\\n",
    "                activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(output2)\n",
    "\n",
    "output3 = Conv2D(filters=20, kernel_size=(1, 1), strides=(1,1), padding='valid', data_format='channels_last', \\\n",
    "                dilation_rate=(1, 1), activation='relu', use_bias=True, kernel_initializer='glorot_uniform', \\\n",
    "                bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, \\\n",
    "                activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(output3)\n",
    "\n",
    "# Max pool layer\n",
    "output1 = GlobalMaxPooling2D(data_format='channels_last')(output1)\n",
    "output2 = GlobalMaxPooling2D(data_format='channels_last')(output2)\n",
    "output3 = GlobalMaxPooling2D(data_format='channels_last')(output3)\n",
    "\n",
    "# Merge\n",
    "output_combined = add([output1,output2,output3])\n",
    "\n",
    "# Final layer\n",
    "output = Dense(1,activation='sigmoid')(output_combined)\n",
    "\n",
    "\n",
    "# build model\n",
    "#model = Model([input_query], [query_output])\n",
    "#model = Model([input_doc], [encoded_doc])\n",
    "#model = Model([input_query,input_doc], [output1,output2,output3])\n",
    "#model = Model([input_query,input_doc], [output_combined])\n",
    "#model = Model([input_query,input_doc], [query_output])\n",
    "#model = Model([input_query,input_doc], [query_output,doc_output])\n",
    "#model = Model([input_query,input_doc], [output])\n",
    "#model = Model([input_query,input_doc,input_exact_match], [mt_input])\n",
    "#model = Model([input_query,input_doc,input_exact_match], [mt_input_1,mt_input_2])\n",
    "model = Model([input_query,input_doc,input_exact_match], [output])\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # from keras.layers import Input, merge\n",
    "# # from keras.models import Model\n",
    "# # import numpy as np\n",
    "\n",
    "# # input_a = np.reshape([[1, 2, 3],[1, 2, 3],],[[1, 2, 3],[1, 2, 3]])\n",
    "# # input_b = np.reshape([4, 5, 6], (1, 1, 3))\n",
    "\n",
    "# # print(input_a)\n",
    "# # print(input_b)\n",
    "# # a = Input(shape=(1, 3))\n",
    "# # b = Input(shape=(1, 3))\n",
    "\n",
    "# # concat = merge([a, b], mode='concat', concat_axis=-1)\n",
    "# # dot = merge([a, b], mode='dot', dot_axes=2)\n",
    "# # cos = merge([a, b], mode='cos', dot_axes=2)\n",
    "\n",
    "# # model_concat = Model(input=[a, b], output=concat)\n",
    "# # model_dot = Model(input=[a, b], output=dot)\n",
    "# # model_cos = Model(input=[a, b], output=cos)\n",
    "\n",
    "# # print(model_concat.predict([input_a, input_b]))\n",
    "# # print(model_dot.predict([input_a, input_b]))\n",
    "# # print(model_cos.predict([input_a, input_b]))\n",
    "\n",
    "# from keras import backend as K\n",
    "# x_batch = K.ones(shape=(11, 6,50 ))\n",
    "# y_batch = K.ones(shape=(11, 400,50))\n",
    "# x_batch=K.batch_flatten(x_batch)\n",
    "# y_batch=K.batch_flatten(y_batch)\n",
    "# #xy_batch_dot = K.dot(transpose(y_batch),x_batch )#, axes=[0,0])\n",
    "# xy_batch_dot = K.batch_dot(x_batch,y_batch, axes=[0,0])\n",
    "# K.int_shape(xy_batch_dot)\n",
    "# # K.int_shape(x_batch)\n",
    "# # K.int_shape(y_batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = optimizers.Adam(lr=0.001)\n",
    "model.compile(optimizer=optimizer, loss='mse', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "x_query_train shape: (74067, 6)\n",
      "x_doc_train shape: (74067, 27)\n",
      "y_train shape: (74067,)\n"
     ]
    }
   ],
   "source": [
    "x_query_train=np.array(query_word2vec_idx_list)[0:74067]\n",
    "x_doc_train=np.array(doc_word2vec_idx_list)[0:74067]#[0:len(x_query_train)]\n",
    "\n",
    "# TODO: replace this with embedding with mask zero rather than padding, need to change index for encoding too, imput dim too.\n",
    "print(\"Pad sequences (samples x time)\")\n",
    "x_query_train = sequence.pad_sequences(x_query_train, maxlen=query_max_len)\n",
    "x_doc_train = sequence.pad_sequences(x_doc_train, maxlen=doc_max_len)\n",
    "# x_query_test = sequence.pad_sequences(x_query_test, maxlen=query_max_len)\n",
    "# x_doc_test = sequence.pad_sequences(x_doc_test, maxlen=doc_max_len)\n",
    "\n",
    "print('x_query_train shape:', x_query_train.shape)\n",
    "print('x_doc_train shape:', x_doc_train.shape)\n",
    "# print('x_query_test shape:', x_query_test.shape)\n",
    "# print('x_doc_test shape:', x_doc_test.shape)\n",
    "\n",
    "#TODO: y label on 0-1 scale\n",
    "y_train=train_query_df['relevance'].as_matrix()/3\n",
    "#y_train=train_query_df['relevance_int'].as_matrix()\n",
    "# y_test=test_query_df['relevance_int'].as_matrix()\n",
    "print('y_train shape:', y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(max(query_word2vec_idx_list,key=len)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exact_match_list=[]\n",
    "for i in range(x_query_train.shape[0]):\n",
    "    #print(i)\n",
    "    c1=x_query_train[i]\n",
    "    c2=x_doc_train[i]\n",
    "    #print(c1.shape[0])\n",
    "    #print(c2.shape[0])\n",
    "\n",
    "    c1_inp=np.repeat(c1,c2.shape[0],axis=0)\n",
    "    c1_inp=c1_inp.reshape((c1.shape[0],c2.shape[0]))\n",
    "    #print(c1_inp.shape)\n",
    "    #print(c1_inp)\n",
    "\n",
    "    #print(c2)\n",
    "    c2_inp=np.tile(c2,(c1.shape[0],1))\n",
    "    #print(c2_inp.shape)\n",
    "    #print(c2_inp)    \n",
    "    #print(c1_inp == c2_inp)\n",
    "    exact_match_list+=[(c1_inp == c2_inp).astype(int)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exact_match_inp_train=np.array(exact_match_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74067, 6, 27)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exact_match_inp_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "angle bracket\n",
      "Simpson Strong-Tie 12-Gauge Angle\n",
      "[   0    0    0 1248  416    0]\n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0 2378  443  179 9680  978   28  315   33  356   77]\n",
      "[[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(train_query_df['search_term'][0])\n",
    "print(product_df['product_title'][0])\n",
    "print(x_query_train[i])\n",
    "print(x_doc_train[i])\n",
    "print(exact_match_inp_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 59253 samples, validate on 14814 samples\n",
      "Epoch 1/3\n",
      "72s - loss: 0.0319 - acc: 0.2899 - val_loss: 0.0345 - val_acc: 0.1316\n",
      "Epoch 2/3\n",
      "70s - loss: 0.0315 - acc: 0.2899 - val_loss: 0.0346 - val_acc: 0.1316\n",
      "Epoch 3/3\n",
      "70s - loss: 0.0314 - acc: 0.2899 - val_loss: 0.0326 - val_acc: 0.1316\n"
     ]
    }
   ],
   "source": [
    "batch_size=100#2\n",
    "print('Train...')\n",
    "hist=model.fit([x_query_train,x_doc_train,exact_match_inp_train], [y_train],\n",
    "          batch_size=batch_size,\n",
    "          epochs=3,\n",
    "          validation_split=0.2,\n",
    "          shuffle=True,\n",
    "          verbose=2,\n",
    "          #validation_data=[x_test, y_test])\n",
    "         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size=100\n",
    "prediction=model.predict([x_query_train,x_doc_train,exact_match_inp_train],\n",
    "          batch_size=batch_size,\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74067, 1)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.79610896],\n",
       "       [ 0.79341847],\n",
       "       [ 0.79600668],\n",
       "       ..., \n",
       "       [ 0.80529708],\n",
       "       [ 0.77157462],\n",
       "       [ 0.79347754]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Predict\")\n",
    "word_a = 'wood'#raw_input('First word: ')\n",
    "if word_a not in word2idx:\n",
    "    print('\"%s\" is not in the index' % word_a)\n",
    "word_b = 'fan'#raw_input('Second word: ')\n",
    "if word_b not in word2idx:\n",
    "    print('\"%s\" is not in the index' % word_b)\n",
    "output = model.predict([np.asarray([word2idx[word_a]]), np.asarray([word2idx[word_b]])])\n",
    "print('%f' % output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
