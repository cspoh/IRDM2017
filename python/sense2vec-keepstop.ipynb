{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "# from keras.preprocessing import sequence\n",
    "# from keras.layers import merge, Dense, Input,Dropout, Embedding, LSTM, Bidirectional, Activation\n",
    "# from keras.layers import Conv2D,Conv1D\n",
    "# from keras.layers.merge import dot, multiply, add, concatenate\n",
    "# from keras.layers import Merge\n",
    "# from keras.layers.core import Lambda,Reshape, Flatten, Dropout\n",
    "# from keras.layers.pooling import GlobalMaxPooling2D, GlobalMaxPooling1D,MaxPooling1D\n",
    "# from keras.models import Model\n",
    "# from keras.backend import transpose,batch_dot,expand_dims\n",
    "# from keras import optimizers\n",
    "from HomeDepotCSVReader import HomeDepotReader\n",
    "import Utilities\n",
    "from DataPreprocessing import DataPreprocessing\n",
    "from Feature_Word2Vec import Feature_Word2Vec\n",
    "from AutomaticQueryExpansion import Word2VecQueryExpansion\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "# from keras.utils.np_utils import to_categorical\n",
    "import pandas as pd\n",
    "from FeatureEngineering import HomeDepotFeature\n",
    "# from keras.layers.wrappers import TimeDistributed\n",
    "# from keras.callbacks import ModelCheckpoint,EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train_filename = '../data/train_play.csv'\n",
    "# test_filename = '../data/test_play.csv'\n",
    "# soln_filename = '../data/solution.csv'\n",
    "# attribute_filename = '../data/attributes_play.csv'\n",
    "# description_filename = '../data/product_descriptions_play.csv'\n",
    "# word2vec_model_path='model/word2vec_play.model'\n",
    "# doc2vec_model_path='model/doc2vec_play.model'\n",
    "# vocab_path='model/word2vec_play_vocab.json'\n",
    "# embeddings_path='model/embeddings_play.npz'\n",
    "\n",
    "train_filename = '../data/train.csv'\n",
    "test_filename = '../data/test.csv'\n",
    "soln_filename = '../data/solution.csv'\n",
    "attribute_filename = '../data/attributes.csv'\n",
    "description_filename = '../data/product_descriptions.csv'\n",
    "word2vec_model_path='model/word2vec_sense2vec_all.model'\n",
    "doc2vec_model_path='model/doc2vec_sense2vec_noun.model'\n",
    "vocab_path='model/doc2vec_sense2vec_vocab_all.json'\n",
    "embeddings_path='model/embeddings_sense2vec_all.npz'\n",
    "full_features_filename = '../data/features_full_plusnouns_pluspuidthresh.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reader = HomeDepotReader()\n",
    "\n",
    "train_query_df, product_df, attribute_df, test_query_df = reader.getQueryProductAttributeDataFrame(train_filename,\n",
    "                                              test_filename,\n",
    "                                              attribute_filename,\n",
    "                                              description_filename)\n",
    "print(\"train_query_df:\",list(train_query_df))\n",
    "print(\"product_df:\", list(product_df))\n",
    "print(\"attribute_df:\", list(attribute_df))\n",
    "print(\"test_query_df:\", list(test_query_df))\n",
    "\n",
    "#transform attribute into doc\n",
    "dp = DataPreprocessing()\n",
    "attribute_doc_df = dp.getAttributeDoc(attribute_df)\n",
    "#attribute_doc_df\n",
    "product_df=product_df.join(attribute_doc_df.set_index('product_uid'), on = 'product_uid')\n",
    "### test data\n",
    "soln_df = pd.read_csv(soln_filename, delimiter=',', low_memory=False, encoding=\"ISO-8859-1\")\n",
    "test_private_df = dp.getGoldTestSet(test_query_df, soln_df, testsetoption='Private')#,savepath='../data/test_private_gold.csv')\n",
    "test_public_df = dp.getGoldTestSet(test_query_df, soln_df, testsetoption='Public')# savepath='../data/test_public_gold.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "product_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_query_df,product_df, attribute_df = HomeDepotFeature().getFeature(train_query_df, product_df, attribute_df, test_private_df,\n",
    "                        features=\"spelling,nonascii,brand\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_query_df,_, _ = HomeDepotFeature().getFeature(test_query_df, product_df, attribute_df, test_private_df,\n",
    "                        features=\"spelling,nonascii\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "product_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ## no attribute\n",
    "product_df['content'] = product_df['product_title'].map(str) + \". \" + \\\n",
    "                        product_df['product_description'].map(str) \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "product_df['content_brand'] = product_df['product_brand']+ \". \" + \\\n",
    "                        product_df['product_title'].map(str) + \". \" + \\\n",
    "                        product_df['product_description'].map(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# attribute\n",
    "product_df['content_attr'] = product_df['attr_json'].map(str)\n",
    "# product_df['product_brand']+ \". \" + \\\n",
    "#                        product_df['product_title'].map(str) + \". \" + \\\n",
    "#                         product_df['product_description'].map(str)  + \". \" + \\\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# #resample remove the low classes\n",
    "# resampled_train_query_df = train_query_df[train_query_df.relevance_int == 0]\n",
    "# min_samples=len(resampled_train_query_df)\n",
    "# for i in range(1,13):\n",
    "#     print(i)\n",
    "#     tmp_df = train_query_df[train_query_df.relevance_int == i]\n",
    "#     if len(tmp_df) > min_samples:\n",
    "#         tmp_df = tmp_df.ix[np.random.choice(tmp_df.index,min_samples)]\n",
    "#         resampled_train_query_df=pd.concat([resampled_train_query_df,tmp_df])\n",
    "\n",
    "#     else:\n",
    "#         print(\"removing {} as too few values {}\".format(i,len(tmp_df)))\n",
    "        \n",
    "# ori_train_query_df=train_query_df        \n",
    "# train_query_df=resampled_train_query_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#don't sample\n",
    "#train_query_df=ori_train_query_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### merge test and train queries for training\n",
    "orig_train_query_df = train_query_df\n",
    "train_query_df = pd.concat([train_query_df,test_query_df])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(train_query_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# end resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "#import sense2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LABELS = {\n",
    "    'ENT': 'ENT',\n",
    "    'PERSON': 'ENT',\n",
    "    'NORP': 'ENT',\n",
    "    'FAC': 'ENT',\n",
    "    'ORG': 'ENT',\n",
    "    'GPE': 'ENT',\n",
    "    'LOC': 'ENT',\n",
    "    'LAW': 'ENT',\n",
    "    'PRODUCT': 'ENT',\n",
    "    'EVENT': 'ENT',\n",
    "    'WORK_OF_ART': 'ENT',\n",
    "    'LANGUAGE': 'ENT',\n",
    "    'DATE': 'DATE',\n",
    "    'TIME': 'TIME',\n",
    "    'PERCENT': 'PERCENT',\n",
    "    'MONEY': 'MONEY',\n",
    "    'QUANTITY': 'QUANTITY',\n",
    "    'ORDINAL': 'ORDINAL',\n",
    "    'CARDINAL': 'CARDINAL',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pre_format_re = re.compile(r'^[\\`\\*\\~]')\n",
    "#post_format_re = re.compile(r'[\\`\\*\\~]$')\n",
    "#url_re = re.compile(r'\\[([^]]+)\\]\\(%%URL\\)')\n",
    "#link_re = re.compile(r'\\[([^]]+)\\]\\(https?://[^\\)]+\\)')\n",
    "comma_num_re = re.compile(r'([0-9])(?:,)([0-9])') #Removed commas between digits (for example, 10, 000 was replaced with 10000)\n",
    "digit_letter_re = re.compile(r'([0-9])([A-Za-z])') #Added dot and space between a digit (left) and a letter (right).\n",
    "letters_digit_re = re.compile(r'([A-Za-z]){3}([0-9])') #Added space between at least three letters (left) and a digit (right).\n",
    "letters_slash_re = re.compile(r'([A-Za-z])(?:[\\\\\\/])([A-Za-z])') #Replaced \\ or / between letters with a space.\n",
    "camel_case_re = re.compile(r'([a-z])([A-Z])')\n",
    "def strip_meta(text,titletext=''):\n",
    "    #text = link_re.sub(r'\\1', text)\n",
    "    #Concatenation of Numbers with Measure Units\n",
    "    text = re.sub('(?<=[0-9])[\\ ]*pound[s]*(?=\\ |$|\\.)', '-lb ', text)\n",
    "    text = re.sub('(?<=[0-9])[\\ ]*lb[s]*(?=\\ |$|\\.)', '-lb ', text)\n",
    "    text = re.sub('(?<=[0-9])[\\ ]*gallon[s]*(?=\\ |$|\\.)', '-gal ', text)\n",
    "    text = re.sub('(?<=[0-9])[\\ ]*gal(?=\\ |$|\\.)', '-gal ', text)\n",
    "    \n",
    "    text = text.replace('&gt;', '>').replace('&lt;', '<').replace('&nbsp;',' ').\\\n",
    "            replace('&amp;','&').replace(';','.').replace(':','.').\\\n",
    "            replace('+',' ').replace('*','x').replace('(','.').replace(')','.')\n",
    "    text = comma_num_re.sub(r'\\1\\2',text)    \n",
    "    text = digit_letter_re.sub(r'\\1. \\2',text)    \n",
    "    text = letters_digit_re.sub(r'\\1 \\2',text)  \n",
    "    text = letters_slash_re.sub(r'\\1 \\2',text)\n",
    "    if titletext:\n",
    "        for word in text.split(): \n",
    "            #print(word)\n",
    "            if camel_case_re.findall(word) and word not in titletext.split():  \n",
    "                word_replace = camel_case_re.sub(r'\\1 \\2',word)\n",
    "                text=re.compile(word).sub(word_replace,text)\n",
    "                #print('here')\n",
    "    #text = pre_format_re.sub('', text)\n",
    "    #text = post_format_re.sub('', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "strip_meta('asd/sd afj\\sjk asasd1100b000 hotDog 10 pounds InSinkErator','InSinkErator asd ads asd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_query_df['search_term'] = train_query_df['search_term'].map(lambda x: strip_meta(x))\n",
    "train_query_df['search_term']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "product_df['content_brand']=product_df.apply(lambda x: strip_meta(text=x['content_brand'],titletext=x['product_title']),axis=1)\n",
    "product_df['content_brand']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "product_df['content_attr']=product_df.apply(lambda x: strip_meta(text=x['content_attr'],titletext=x['product_title']),axis=1)\n",
    "product_df['content_attr']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "#nlp = spacy.load('en')\n",
    "#import en_vectors_glove_md\n",
    "#nlp = en_vectors_glove_md.load()\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentence='BEHR_Premium_Textured_deck|NOUN over|ADP 1-gal|VERB SC|NOUN 141|CARDINAL Tugboat_Wood|ENT'\n",
    "keep_tags=['NOUN','PROPN','ENT','NUM','DATE','TIME','PERCENT','MONEY','QUANTITY','ORDINAL','CARDINAL']\n",
    "#a=re.compile('^(\\|'+'|'.join(keep_tags)+')')\n",
    "keep_tags_re=re.compile('((?:[A-Za-z0-9_]+?\\|)(?:NOUN|PROPN|ENT|NUM|DATE|TIME|PERCENT|MONEY|QUANTITY|ORDINAL|CARDINAL))')\n",
    "print(sentence)\n",
    "print(' '.join(keep_tags_re.findall(sentence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def represent_word(word):\n",
    "    if word.like_url:\n",
    "        return '%%URL|X'\n",
    "    text = re.sub(r'\\s', '_', word.text)\n",
    "    tag = LABELS.get(word.ent_type_, word.pos_)\n",
    "    if not tag:\n",
    "        tag = '?'\n",
    "    return text + '|' + tag\n",
    "\n",
    "keep_tags_re=re.compile('((?:[A-Za-z0-9_]+?\\|)(?:NOUN|PROPN|ENT|NUM|DATE|TIME|PERCENT|MONEY|QUANTITY|ORDINAL|CARDINAL))')\n",
    "def transform_doc(doc,filter_pos=False):\n",
    "    for ent in doc.ents:\n",
    "        ent.merge(ent.root.tag_, ent.text, LABELS[ent.label_])\n",
    "    for np in doc.noun_chunks:\n",
    "        while len(np) > 1 and np[0].dep_ not in ('advmod', 'amod', 'compound'):\n",
    "            np = np[1:]\n",
    "        np.merge(np.root.tag_, np.text, np.root.ent_type_)\n",
    "    strings = []\n",
    "    for sent in doc.sents:\n",
    "        if sent.text.strip():         \n",
    "            if filter_pos == True:\n",
    "                s = ' '.join(represent_word(w) for w in sent if not w.is_space)\n",
    "                strings.append(' '.join(keep_tags_re.findall(s)))\n",
    "            else:\n",
    "                strings.append(' '.join(represent_word(w) for w in sent if not w.is_space))\n",
    "    if strings:\n",
    "        return '\\n'.join(strings) + '\\n'\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from Utilities import Utility\n",
    "#timetracker=Utility()\n",
    "\n",
    "i=0\n",
    "for row in product_df.itertuples():\n",
    "    #timetracker.startTimeTrack()\n",
    "    #start_time = time.time()\n",
    "    print(row.content_brand)\n",
    "    print(\"=========\")\n",
    "    doc=nlp(row.content_brand)\n",
    "    print(transform_doc(doc,filter_pos=False))\n",
    "    i+=1\n",
    "    #timetracker.checkpointTimeTrack()\n",
    "    #print(\"%s s\" % round(((time.time() - start_time) ), 2))\n",
    "    if i > 1:\n",
    "        break\n",
    "\n",
    "# product_df['sense2vec_content_attr']=product_df.apply(lambda x: transform_doc(nlp(x['content_attr'])),axis=1)\n",
    "# product_df['sense2vec_content_attr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "    \n",
    "# what are your inputs, and what operation do you want to \n",
    "# perform on each input. For example...\n",
    "def processInput(i):\n",
    "    return transform_doc(nlp(i),filter_pos=False)\n",
    "\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "    \n",
    "results = Parallel(n_jobs=num_cores)(delayed(processInput)(i.content_brand) for i in product_df.itertuples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sense2vec_content_brand_df=pd.DataFrame(results,columns=['sense2vec_content_brand'])\n",
    "product_df=pd.concat([product_df,sense2vec_content_brand_df],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "product_df['sense2vec_content_brand'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# product_df['sense2vec_content_brand']=product_df.apply(lambda x: transform_doc(nlp(x['content_brand'])),axis=1)\n",
    "# product_df['sense2vec_content_brand']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_query_df['sense2vec_search_term']=train_query_df.apply(lambda x: transform_doc(nlp(x['search_term']),filter_pos=False),axis=1)\n",
    "train_query_df['sense2vec_search_term']\n",
    "\n",
    "# results = Parallel(n_jobs=num_cores)(delayed(processInput)(i.search_term) for i in train_query_df.itertuples())\n",
    "# sense2vec_search_term_df=pd.DataFrame(results,columns=['sense2vec_search_term'])\n",
    "# train_query_df=pd.concat([train_query_df,sense2vec_search_term_df],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_query_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prep for word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences = []  # Initialize an empty list of sentences\n",
    "query_sentences = []\n",
    "doc_sentences = []\n",
    "\n",
    "print(\"Parsing sentences from search string\")\n",
    "for query in train_query_df[\"sense2vec_search_term\"]:\n",
    "    query_sentences += [query.split()]\n",
    "\n",
    "print(\"Parsing sentences from unlabeled set\")\n",
    "for doc in product_df['sense2vec_content_brand']:\n",
    "    doc_sentences += [doc.split()]\n",
    "\n",
    "sentences = query_sentences+doc_sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences[-3:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import the built-in logging module and configure it so that Word2Vec \n",
    "# creates nice output messages\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 300#50#300    # Word vector dimensionality                      \n",
    "min_word_count = 1#5#40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-5 #0 #1e-3   # Downsample setting for frequent words\n",
    "iterations = 10\n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "from gensim.models import word2vec\n",
    "print(\"Training model...\")\n",
    "word2vec_model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling, iter=iterations)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "word2vec_model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "#model_name = \"300features_40minwords_10context\"\n",
    "word2vec_model.save(word2vec_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(word2vec_model.wv.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab_words=[k for k,v in word2vec_model.wv.vocab.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(word2vec_model.most_similar('wood|NOUN', [], 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(word2vec_model.most_similar('Whirlpool|ENT', [], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(word2vec_model.most_similar('kitchen|NOUN', [], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(word2vec_model.most_similar('shower|NOUN', [], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(word2vec_model.most_similar('steel|NOUN', [], 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(word2vec_model.most_similar('microwave|NOUN', [], 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(word2vec_model.most_similar('LED|NOUN', [], 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(word2vec_model.most_similar('stool|NOUN', [], 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "i=0\n",
    "#all_similarity_scores=[]\n",
    "#keep_tag_similarity_scores=[]\n",
    "avg_all_similarity_scores=[]\n",
    "avg_keep_tag_similarity_scores=[]\n",
    "avg_uidfact_all_similarity_scores=[]\n",
    "avg_uidfact_keep_tag_similarity_scores=[]\n",
    "relevance_scores=[]\n",
    "for query_row in train_query_df.itertuples():    \n",
    "    query=nlp(str(query_row.search_term)) \n",
    "    relevance=query_row.relevance\n",
    "    product_uid=int(query_row.product_uid)\n",
    "    #product_idx=query_row.product_idx.values[0]\n",
    "    product_idx=product_df[product_df['product_uid'] == product_uid]['content_brand'].index[0] #feature_df workaround\n",
    "    \n",
    "    if product_df.iloc[product_idx]['product_uid'] < 163100:\n",
    "        product_uid_factor=1.0\n",
    "    else:\n",
    "        product_uid_factor=0.75\n",
    "        \n",
    "    ## similarity doc\n",
    "    #similarity_score = doc2vec_model.docvecs.similarity(i,int(len(train_query_df)+product_idx))\n",
    "    \n",
    "    ### cosine\n",
    "    keep_tag_similarity_score=0\n",
    "    all_similarity_score=0\n",
    "    uidfact_keep_tag_similarity_score=0\n",
    "    uidfact_all_similarity_score=0\n",
    "    for word in query_row.sense2vec_search_term.split():        \n",
    "        #print(word2vec_model.most_similar(word, [], 5))\n",
    "        for doc_word in product_df.iloc[product_idx]['sense2vec_content_brand'].split():\n",
    "            similarity = word2vec_model.similarity(word,doc_word)\n",
    "            #print(\"{:.2f} = {} {} to {} \".format(similarity,relevance,word,doc_word))\n",
    "            if keep_tags_re.findall(doc_word):\n",
    "                keep_tag_similarity_score+=similarity\n",
    "                uidfact_keep_tag_similarity_score+=similarity*product_uid_factor\n",
    "                #print('keep_tag score = {}'.format(keep_tag_similarity_score))\n",
    "            all_similarity_score+=similarity\n",
    "            uidfact_all_similarity_score+=similarity*product_uid_factor\n",
    "        \n",
    "        #print(doc2vec_model[word].reshape(-1,1).T.shape)\n",
    "        #print(doc2vec_model.docvecs[int(len(train_query_df)+product_idx)].reshape(-1,1).T.shape)\n",
    "#         similarity_score += np.squeeze(cosine_similarity(doc2vec_model[word].reshape(-1,1).T,\\\n",
    "#                           doc2vec_model.docvecs[int(len(train_query_df)+product_idx)].reshape(-1,1).T))\n",
    "#         print(np.squeeze(cosine_similarity(doc2vec_model[word].reshape(-1,1).T,\\\n",
    "#                          doc2vec_model.docvecs[int(len(train_query_df)+product_idx)].reshape(-1,1).T)))\n",
    "#       print(similarity_score)\n",
    "\n",
    "    ### end cosine\n",
    "    avg_all_similarity_scores+=[all_similarity_score/(len(query_row.sense2vec_search_term.split())*len(product_df.iloc[product_idx]['sense2vec_content_brand'].split()))]\n",
    "    avg_keep_tag_similarity_scores+=[keep_tag_similarity_score/(len(query_row.sense2vec_search_term.split())*len(product_df.iloc[product_idx]['sense2vec_content_brand'].split()))]\n",
    "    avg_uidfact_all_similarity_scores+=[uidfact_all_similarity_score/(len(query_row.sense2vec_search_term.split())*len(product_df.iloc[product_idx]['sense2vec_content_brand'].split()))]\n",
    "    avg_uidfact_keep_tag_similarity_scores+=[uidfact_keep_tag_similarity_score/(len(query_row.sense2vec_search_term.split())*len(product_df.iloc[product_idx]['sense2vec_content_brand'].split()))]\n",
    "    #all_similarity_scores+=[all_similarity_score]\n",
    "    #keep_tag_similarity_scores+=[keep_tag_similarity_score]\n",
    "    relevance_scores+=[relevance]\n",
    "    \n",
    "    i+=1\n",
    "    if i % 1000 == 0: #print every 100 \n",
    "        print(i)        \n",
    "#     if i >= 200:\n",
    "#         print(relevance,all_similarity_score,keep_tag_similarity_score)\n",
    "#         break\n",
    "#     else:\n",
    "#         print(relevance,all_similarity_score,keep_tag_similarity_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.gcf()\n",
    "fig.set_size_inches(8,15)\n",
    "#sns.stripplot(relevance_scores,similarity_scores,jitter=True, alpha=.40)\n",
    "#sns.pointplot(relevances,overlap_ratios,jitter=True, alpha=.40)\n",
    "sns.boxplot(relevance_scores,avg_uidfact_all_similarity_scores)\n",
    "\n",
    "#sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.gcf()\n",
    "fig.set_size_inches(8,15)\n",
    "#sns.stripplot(relevance_scores,similarity_scores,jitter=True, alpha=.40)\n",
    "#sns.pointplot(relevances,overlap_ratios,jitter=True, alpha=.40)\n",
    "sns.boxplot(relevance_scores,avg_uidfact_keep_tag_similarity_scores)\n",
    "\n",
    "#sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.gcf()\n",
    "fig.set_size_inches(8,15)\n",
    "#sns.stripplot(relevance_scores,similarity_scores,jitter=True, alpha=.40)\n",
    "#sns.pointplot(relevances,overlap_ratios,jitter=True, alpha=.40)\n",
    "sns.boxplot(relevance_scores,avg_all_similarity_scores)\n",
    "\n",
    "#sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.gcf()\n",
    "fig.set_size_inches(8,15)\n",
    "#sns.stripplot(relevance_scores,similarity_scores,jitter=True, alpha=.40)\n",
    "#sns.pointplot(relevances,overlap_ratios,jitter=True, alpha=.40)\n",
    "sns.boxplot(relevance_scores,avg_keep_tag_similarity_scores)\n",
    "\n",
    "#sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id_df = pd.DataFrame(train_query_df['id'],index=train_query_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ref_index=train_query_df.index\n",
    "new_feature_df1 = pd.DataFrame(avg_all_similarity_scores,columns=['sense2vec_all_simscore'],index=ref_index)\n",
    "new_feature_df2 = pd.DataFrame(avg_keep_tag_similarity_scores,columns=['sense2vec_keeptag_simscore'],index=ref_index)\n",
    "new_feature_df3 = pd.DataFrame(avg_uidfact_all_similarity_scores,columns=['sense2vec_uidfact_all_simscore'],index=ref_index)\n",
    "new_feature_df4 = pd.DataFrame(avg_uidfact_keep_tag_similarity_scores,columns=['sense2vec_uidfact_keeptag_simscore'],index=ref_index)\n",
    "new_feature_df = pd.concat([id_df,new_feature_df1,new_feature_df2,new_feature_df3,new_feature_df4],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(new_feature_df).to_csv('../data/features_sense2vec_simscore_attr.csv', \\\n",
    "                                    index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save_all_similarity_scores = all_similarity_scores\n",
    "# save_keep_tag_similarity_scores = keep_tag_similarity_scores\n",
    "# save_avg_all_similarity_scores=avg_all_similarity_scores\n",
    "# save_avg_keep_tag_similarity_scores=avg_keep_tag_similarity_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# concat to full features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_df = reader.getBasicDataFrame(full_features_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "copy_new_feature_df = new_feature_df.copy()\n",
    "_ = copy_new_feature_df.pop('id')\n",
    "copy_new_feature_df.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "copy_new_feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_df2 = pd.concat([feature_df,copy_new_feature_df],axis=1)\n",
    "#copy_new_feature_df\n",
    "#feature_df2 = feature_df.join(new_feature_df,on='id',how='inner',rsuffix='rid_')\n",
    "\n",
    "#feature_df2=feature_df.set_index('id').join(feature_df2.set_index('id'),lsuffix='_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#_=feature_df2.pop('idrid_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(feature_df2).to_csv('../data/features_full_plusnouns_pluspuidthreshpluss2vsimscorev2.csv', \\\n",
    "                                    index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# concat to latests features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "latest_feature_df = reader.getBasicDataFrame('../data/features_doc2vec_sense2vec_20170416.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "latest_feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_df[['id','product_uid_threshold']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_df3 = pd.concat([latest_feature_df,copy_new_feature_df],axis=1)\n",
    "\n",
    "# feature_df3 = latest_feature_df.join(new_feature_df,on='id',how='inner',rsuffix='rid_')\n",
    "# _=feature_df3.pop('idrid_')\n",
    "# feature_df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_df3 = pd.concat([feature_df3,feature_df['product_uid_threshold']],axis=1)\n",
    "\n",
    "# feature_df3 = feature_df3.join(feature_df[['id','product_uid_threshold']],on='id',how='inner',rsuffix='rid_')\n",
    "# _=feature_df3.pop('idrid_')\n",
    "# feature_df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(feature_df3).to_csv('../data/features_doc2vec_sense2vec_20170417.csv', \\\n",
    "                                    index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prep for doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences = []  # Initialize an empty list of sentences\n",
    "query_sentences = []\n",
    "doc_sentences = []\n",
    "\n",
    "print(\"Parsing sentences from search string\")\n",
    "for query in train_query_df[\"sense2vec_search_term\"]:\n",
    "    query_sentences += [query.split()]\n",
    "\n",
    "print(\"Parsing sentences from unlabeled set\")\n",
    "for doc in product_df['sense2vec_content_brand']:\n",
    "    doc_sentences += [doc.split()]\n",
    "\n",
    "sentences = query_sentences+doc_sentences    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences[-3:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(sentences))\n",
    "\n",
    "from gensim.models import doc2vec #word2vec\n",
    "\n",
    "documents=[]\n",
    "for i,s in enumerate(sentences):\n",
    "    if s:\n",
    "        document={}\n",
    "        document= doc2vec.LabeledSentence(words=s,tags=['SENT_'+str(i)]) \n",
    "        documents+=[document]\n",
    "print(documents[-1])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import the built-in logging module and configure it so that Word2Vec \n",
    "# creates nice output messages\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 100#50#300    # Word vector dimensionality                      \n",
    "min_word_count = 1#5#40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 10e-5 #0 #1e-3   # Downsample setting for frequent words\n",
    "\n",
    "#https://github.com/jhlau/doc2vec hyperparam guide\n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "from gensim.models import doc2vec #word2vec\n",
    "print(\"Training model...\")\n",
    "# doc2vec_model = doc2vec.Doc2Vec(documents, workers=num_workers, \\\n",
    "#             size=num_features, min_count = min_word_count, \\\n",
    "#             window = context, sample = downsampling,iter=20)\n",
    "\n",
    "# doc2vec_model = doc2vec.Doc2Vec(documents, size=300, min_count=5, window=5, \\\n",
    "#                             iter=30, workers=4, alpha=0.1, min_alpha=0.0001,\n",
    "#                         dm_concat=1, dm=0, negative=5)\n",
    "\n",
    "# 20:43 run\n",
    "# doc2vec_model = doc2vec.Doc2Vec(documents, size=100, min_count=0, window=8, \\\n",
    "#                             iter=20, workers=4, #alpha=0.1, min_alpha=0.0001,\n",
    "#                             dm_concat=1, dm=0,sample = 0,  negative=5,dbow_words=1)  \n",
    "\n",
    "# # 20:57 run\n",
    "# doc2vec_model = doc2vec.Doc2Vec(documents, size=100, min_count=0, window=16, \\\n",
    "#                             iter=20, workers=4, #alpha=0.1, min_alpha=0.0001,\n",
    "#                             dm_concat=1, dm=0,sample = downsampling,  negative=5,dbow_words=1)  \n",
    "\n",
    "#New\n",
    "doc2vec_model = doc2vec.Doc2Vec(documents, size=200, min_count=1, window=10, \\\n",
    "                            iter=30, workers=4, #alpha=0.1, min_alpha=0.0001,\n",
    "                            sample = downsampling,  negative=5,dbow_words=1,\n",
    "                               dm_mean=0, dm_concat=1, dm=1,)  \n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "doc2vec_model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "#model_name = \"300features_40minwords_10context\"\n",
    "doc2vec_model.save(doc2vec_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(doc2vec_model.wv.vocab))\n",
    "doc2vec_model.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab_words=[k for k,v in doc2vec_model.wv.vocab.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_query_df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r = re.compile(\"r_emote\\|\",flags=re.IGNORECASE)\n",
    "newlist = filter(r.match, vocab_words)\n",
    "print(list(newlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(doc2vec_model.most_similar('r_emote|NOUN', [], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(doc2vec_model.docvecs.most_similar( [ doc2vec_model['r_emote|NOUN'] ] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(sentences[0])\n",
    "print(sentences[len(train_query_df)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc2vec_model.docvecs.most_similar(['SENT_0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(sentences[74067])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#doc2vec_model.docvecs.similarity('SENT_17','SENT_74068')\n",
    "doc2vec_model.docvecs.similarity(0,74067+11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "len(train_query_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_query_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "i=0\n",
    "similarity_scores=[]\n",
    "relevance_scores=[]\n",
    "for query_row in train_query_df.itertuples():    \n",
    "    query=nlp(str(query_row.search_term)) \n",
    "    relevance=query_row.relevance\n",
    "    product_uid=int(query_row.product_uid)\n",
    "    #product_idx=query_row.product_idx.values[0]\n",
    "    product_idx=product_df[product_df['product_uid'] == product_uid]['content_attr'].index[0] #feature_df workaround\n",
    "    \n",
    "    ## similarity doc\n",
    "    similarity_score = doc2vec_model.docvecs.similarity(i,int(len(train_query_df)+product_idx))\n",
    "    \n",
    "    ### cosine\n",
    "#     similarity_score=0\n",
    "#     for word in query_row.sense2vec_search_term.split():\n",
    "#         #print(word)\n",
    "#         #print(doc2vec_model[word].reshape(-1,1).T.shape)\n",
    "#         #print(doc2vec_model.docvecs[int(len(train_query_df)+product_idx)].reshape(-1,1).T.shape)\n",
    "#         similarity_score += np.squeeze(cosine_similarity(doc2vec_model[word].reshape(-1,1).T,\\\n",
    "#                           doc2vec_model.docvecs[int(len(train_query_df)+product_idx)].reshape(-1,1).T))\n",
    "# #         print(np.squeeze(cosine_similarity(doc2vec_model[word].reshape(-1,1).T,\\\n",
    "# #                          doc2vec_model.docvecs[int(len(train_query_df)+product_idx)].reshape(-1,1).T)))\n",
    "# #       print(similarity_score)\n",
    "\n",
    "    ### end cosine\n",
    "    \n",
    "    similarity_scores+=[similarity_score]\n",
    "    relevance_scores+=[relevance]\n",
    "    \n",
    "    i+=1\n",
    "    if i % 1000 == 0: #print every 100 \n",
    "        print(i)\n",
    "#     if i >= 10:\n",
    "#         break\n",
    "#     else:\n",
    "#         print(relevance,similarity_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.gcf()\n",
    "fig.set_size_inches(8,15)\n",
    "#sns.stripplot(relevance_scores,similarity_scores,jitter=True, alpha=.40)\n",
    "#sns.pointplot(relevances,overlap_ratios,jitter=True, alpha=.40)\n",
    "sns.boxplot(relevance_scores,similarity_scores)\n",
    "\n",
    "#sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from gensim.models.word2vec import Word2Vec\n",
    "# from gensim.models.word2vec import KeyedVectors\n",
    "# word2vec_model = Word2Vec.load('/home/ongmin/PycharmProjects/IRDM2017/data/models/prod_content_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#word2vec_model.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(doc2vec_model.most_similar('Whirlpool|ENT', [], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(doc2vec_model.most_similar('wood|NOUN', [], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(doc2vec_model.most_similar('kitchen|NOUN', [], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(doc2vec_model.most_similar('shower|NOUN', [], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(doc2vec_model.most_similar('steel|NOUN', [], 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(doc2vec_model.most_similar('microwave|NOUN', [], 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(doc2vec_model.most_similar('LED|NOUN', [], 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words=[k for k,v in word2vec_model.wv.vocab.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r = re.compile(\"stool\\|\",flags=re.IGNORECASE)\n",
    "newlist = filter(r.match, words)\n",
    "print(list(newlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for word in list(filter(r.match, words)):\n",
    "    #print(str(word))\n",
    "    print(word,word2vec_model.most_similar(word, [], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "nlp.vocab.load_vectors(\"/home/ongmin/PycharmProjects/IRDM2017/data/models/prod_content_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sense2vec.vectors import VectorMap\n",
    "vector_map = VectorMap(300)\n",
    "a=vector_map.load('/home/ongmin/PycharmProjects/IRDM2017/data/models/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = sense2vec.load('/home/ongmin/PycharmProjects/IRDM2017/data/models/prod_content_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
